{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDnA7kr2eE5hbYDWuayq/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HebatallaTarek/Empathy-Mental-Health/blob/master/Pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iZ7h1FcSfXA"
      },
      "source": [
        "\n",
        "**bold text**\n",
        "# Configurations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMAnFq7Q4pat"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ekpHU_S7iTl",
        "outputId": "e85d1d38-dc5a-44f4-f4e2-a4f2ed18e7fb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgUTqJXhBEO7"
      },
      "source": [
        "#Set the path to the data folder, datafile and output folder and files\n",
        "root_folder = '/content/drive/My Drive'\n",
        "data_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/empathy_dataset'))\n",
        "model_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/pre-training model'))\n",
        "output_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/pre-tarining output'))\n",
        "tokenizer_folder = os.path.abspath(os.path.join(root_folder, 'Empathy Detection Project/tokenizer'))\n",
        "\n",
        "test_filename='emotional-reactions_Test-reddit.csv'\n",
        "datafile= 'emotional-reactions-reddit.csv'\n",
        "outputfile = 'emotionalreactionafterMLM.csv'\n",
        "\n",
        "datafile_path = os.path.abspath(os.path.join(data_folder,datafile))\n",
        "testfile_path = os.path.abspath(os.path.join(data_folder,test_filename))\n",
        "outputfile_path = os.path.abspath(os.path.join(output_folder,outputfile))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XTJ87ycUDuy"
      },
      "source": [
        "# Load Training and Testing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtfdxU5zH983",
        "outputId": "fcad7f87-5610-49ef-fdc5-a637cba3eaa9"
      },
      "source": [
        "# Load the train dataset\n",
        "train_df=pd.read_csv(datafile_path, header=0, usecols=[2,3])\n",
        "# Show the count of rows\n",
        "print('Num Examples: ',len(train_df))\n",
        "print('Null Values\\n', train_df.isna().sum())\n",
        "print(train_df)\n",
        "# Drop rows with Null values \n",
        "train_df.dropna(inplace=True)\n",
        "print('Num Examples: ',len(train_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Examples:  3084\n",
            "Null Values\n",
            " seeker_post      0\n",
            "response_post    0\n",
            "dtype: int64\n",
            "                                            seeker_post                                      response_post\n",
            "0         Help. Help me. I dunno what I'm doing anymore  That's pretty vague, do you not know what you'...\n",
            "1     I'm done saying I love you to her because I do...  idk what a Red pill means exactly but my advic...\n",
            "2     Always feel like I'm being criticized and mock...  I think it's social anxiety , that creates par...\n",
            "3     My diet becomes fucked when i get depressed.. ...  By any chance do you think you're in a loop. J...\n",
            "4     I hate not knowing why. I was diagnosed with d...  depression. not sadness which is caused by som...\n",
            "...                                                 ...                                                ...\n",
            "3079  does anyone else keep forgetting stuff the nee...  All day, every day. It's definitely not just y...\n",
            "3080  What does depression feel like?. Honest questi...  like being stuck in a black hole. At times you...\n",
            "3081  I'm to scared to commit suicide.. All I can fe...  I probably would have considered bringing harm...\n",
            "3082  I just want to disappear but I don't want to h...                        People barely notice me too\n",
            "3083  26 year old male, living at home, low income j...  I see what you mean, but imagine being 26, liv...\n",
            "\n",
            "[3084 rows x 2 columns]\n",
            "Num Examples:  3084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJb9myAER2F0",
        "outputId": "379d28ab-4054-4fb8-98f5-e1823f759dd5"
      },
      "source": [
        "# Load the test dataset \n",
        "test_df=pd.read_csv(testfile_path, header=0)\n",
        "print('Num Examples: ',len(test_df))\n",
        "print('Null Values\\n', test_df.isna().sum())\n",
        "print(test_df)\n",
        "# there are no null values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Examples:  1001\n",
            "Null Values\n",
            " seeker_post      0\n",
            "response_post    0\n",
            "dtype: int64\n",
            "                                            seeker_post                                      response_post\n",
            "0     Suicide. I think about suicide every day. Been...  Do you know the author Tom Robbins? He recentl...\n",
            "1     I hate when people say that when I kill myself...  Thanks for sharing that. I agree that telling ...\n",
            "2     Does anyone notice a change in libido during e...  Now this is an odd one. I felt a drop in my se...\n",
            "3     I donâ€™t have the ability to cry anymore. No ma...  I experienced smth similar during the last two...\n",
            "4     Do happy occasions make anyone else feel more ...  I always see couples and hate them. Not becaus...\n",
            "...                                                 ...                                                ...\n",
            "996   does anyone else keep forgetting stuff the nee...  All day, every day. It's definitely not just y...\n",
            "997   What does depression feel like?. Honest questi...  like being stuck in a black hole. At times you...\n",
            "998   I'm to scared to commit suicide.. All I can fe...  I probably would have considered bringing harm...\n",
            "999   I just want to disappear but I don't want to h...                        People barely notice me too\n",
            "1000  26 year old male, living at home, low income j...  I see what you mean, but imagine being 26, liv...\n",
            "\n",
            "[1001 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMKwdXOyUQKJ"
      },
      "source": [
        "# Create the dataset to train a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7VCYgRPSaWN"
      },
      "source": [
        "\n",
        "\n",
        "# Drop the files from the output dir\n",
        "txt_files_dir = \"./text_split\"\n",
        "!rm -rf {txt_files_dir}\n",
        "!mkdir {txt_files_dir}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QZX4wXcVbWp"
      },
      "source": [
        "# Store values in a dataframe column (Series object) to files, one file per record\n",
        "def column_to_files(column, prefix, txt_files_dir):\n",
        "    # The prefix is a unique ID to avoid to overwrite a text file\n",
        "    i=prefix\n",
        "    #For every value in the df, with just one column\n",
        "    for row in column.to_list():\n",
        "      # Create the filename using the prefix ID\n",
        "      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
        "      try:\n",
        "        # Create the file and write the column text to it\n",
        "        f = open(file_name, 'wb')\n",
        "        f.write(row.encode('utf-8'))\n",
        "        f.close()\n",
        "      except Exception as e:  #catch exceptions(for eg. empty rows)\n",
        "        print(row, e) \n",
        "      i+=1\n",
        "    # Return the last ID\n",
        "    return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSOL5uOVtO0",
        "outputId": "9cc75f7a-f0af-46d9-da50-acd1dd496405"
      },
      "source": [
        "data = train_df[\"response_post\"]\n",
        "# Removing the end of line character \\n\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "# Set the ID to 0\n",
        "prefix=0\n",
        "# Create a file for every description value\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "# Print the last ID\n",
        "print(prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evk2ZquEZYXK",
        "outputId": "af544395-f622-46f8-e717-80110506ff0f"
      },
      "source": [
        "data = test_df[\"response_post\"]\n",
        "# Removing the end of line character \\n\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "print(len(data))\n",
        "# Create a file for every description value\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "print(prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001\n",
            "4085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfJZjRPHwP6_",
        "outputId": "9d888633-768d-4628-9c1f-d9ff82479844"
      },
      "source": [
        "data = train_df[\"seeker_post\"]\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "print(len(data))\n",
        "prefix = column_to_files(data, prefix, txt_files_dir)\n",
        "print(prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3084\n",
            "7169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBuGo7Ddkc3-"
      },
      "source": [
        "# Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkd9Wf83X5H6",
        "outputId": "41694e9a-8bcc-4ba8-f219-f1295395e0f4"
      },
      "source": [
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1\n",
        "import tensorflow as tf \n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-q6r9dpi5\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-q6r9dpi5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuxsT0z5j2s9"
      },
      "source": [
        "paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
        "                show_progress=True,\n",
        "                special_tokens=[\n",
        "                                \"<s>\",\n",
        "                                \"<pad>\",\n",
        "                                \"</s>\",\n",
        "                                \"<unk>\",\n",
        "                                \"<mask>\",\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b13M1o4llUhu",
        "outputId": "593cd9b9-f364-4d6e-e51a-a8ccfbe32b37"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=8192, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9grSpVpl6l9",
        "outputId": "35fa0494-343b-44ac-9c71-a5b854fede03"
      },
      "source": [
        "# Tokenizer. save_model () is outdated so we used .save_pretrained()\n",
        "tokenizer.save_pretrained(tokenizer_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/vocab.json',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/merges.txt',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/added_tokens.json',\n",
              " '/content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsIqN5LdqXyS"
      },
      "source": [
        "# Create the tokenizer using vocab.json and mrege.txt files\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
        "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hOcwwQztDBZ"
      },
      "source": [
        "# Prepare the tokenizer\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruFAQlf-t2h2",
        "outputId": "a7dfb98c-e72d-4769-ae11-8246eb5d6550"
      },
      "source": [
        "tokenizer.encode(\"Do you know the author Tom Robbins? He recently wrote a memoir. In it, he said Everything can change completely at any moment. This is still the best argument I know against suicide. You still have plenty of life to reap, I guarantee it. What you're going through is temporary, even if it's been a long time, even if it seems like there's no end in sight. You're going to be ok.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=116, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kthKBnPKuWq6",
        "outputId": "027e99aa-6523-4878-ba68-7a92baa03370"
      },
      "source": [
        "tokenizer.encode(\"Do you know the author Tom Robbins? He recently wrote a memoir. In it, he said Everything can change completely at any moment. This is still the best argument I know against suicide. You still have plenty of life to reap, I guarantee it. What you're going through is temporary, even if it's been a long time, even if it seems like there's no end in sight. You're going to be ok.\").tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'D',\n",
              " 'o',\n",
              " 'Ä you',\n",
              " 'Ä know',\n",
              " 'Ä the',\n",
              " 'Ä author',\n",
              " 'Ä ',\n",
              " 'T',\n",
              " 'om',\n",
              " 'Ä ',\n",
              " 'R',\n",
              " 'o',\n",
              " 'bb',\n",
              " 'ins',\n",
              " '?',\n",
              " 'Ä ',\n",
              " 'H',\n",
              " 'e',\n",
              " 'Ä recently',\n",
              " 'Ä wrote',\n",
              " 'Ä a',\n",
              " 'Ä me',\n",
              " 'mo',\n",
              " 'ir',\n",
              " '.',\n",
              " 'Ä ',\n",
              " 'I',\n",
              " 'n',\n",
              " 'Ä it',\n",
              " ',',\n",
              " 'Ä he',\n",
              " 'Ä said',\n",
              " 'Ä ',\n",
              " 'E',\n",
              " 'very',\n",
              " 'thing',\n",
              " 'Ä can',\n",
              " 'Ä change',\n",
              " 'Ä completely',\n",
              " 'Ä at',\n",
              " 'Ä any',\n",
              " 'Ä moment',\n",
              " '.',\n",
              " 'Ä ',\n",
              " 'T',\n",
              " 'h',\n",
              " 'is',\n",
              " 'Ä is',\n",
              " 'Ä still',\n",
              " 'Ä the',\n",
              " 'Ä best',\n",
              " 'Ä argument',\n",
              " 'Ä ',\n",
              " 'I',\n",
              " 'Ä know',\n",
              " 'Ä against',\n",
              " 'Ä suicide',\n",
              " '.',\n",
              " 'Ä ',\n",
              " 'Y',\n",
              " 'ou',\n",
              " 'Ä still',\n",
              " 'Ä have',\n",
              " 'Ä plenty',\n",
              " 'Ä of',\n",
              " 'Ä life',\n",
              " 'Ä to',\n",
              " 'Ä reap',\n",
              " ',',\n",
              " 'Ä ',\n",
              " 'I',\n",
              " 'Ä guarantee',\n",
              " 'Ä it',\n",
              " '.',\n",
              " 'Ä ',\n",
              " 'W',\n",
              " 'hat',\n",
              " 'Ä you',\n",
              " \"'re\",\n",
              " 'Ä going',\n",
              " 'Ä through',\n",
              " 'Ä is',\n",
              " 'Ä temporary',\n",
              " ',',\n",
              " 'Ä even',\n",
              " 'Ä if',\n",
              " 'Ä it',\n",
              " \"'s\",\n",
              " 'Ä been',\n",
              " 'Ä a',\n",
              " 'Ä long',\n",
              " 'Ä time',\n",
              " ',',\n",
              " 'Ä even',\n",
              " 'Ä if',\n",
              " 'Ä it',\n",
              " 'Ä seems',\n",
              " 'Ä like',\n",
              " 'Ä there',\n",
              " \"'s\",\n",
              " 'Ä no',\n",
              " 'Ä end',\n",
              " 'Ä in',\n",
              " 'Ä sight',\n",
              " '.',\n",
              " 'Ä ',\n",
              " 'Y',\n",
              " 'ou',\n",
              " \"'re\",\n",
              " 'Ä going',\n",
              " 'Ä to',\n",
              " 'Ä be',\n",
              " 'Ä ok',\n",
              " '.',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVAQaPYwfOV"
      },
      "source": [
        "# Train Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guM09-c1wkki"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 8   # input batch size for training (default: 64)\n",
        "VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
        "TRAIN_EPOCHS = 13       # number of epochs to train (default: 10)\n",
        "LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n",
        "WEIGHT_DECAY = 0.01\n",
        "SEED = 12               # random seed (default: 42)\n",
        "MAX_LEN = 128\n",
        "SUMMARY_LEN = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtTmnpb0xh0h",
        "outputId": "a8882ec2-51ff-4aee-a085-6fa38ed54ebf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 12 02:53:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    74W / 149W |    144MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLrmdFXzidz",
        "outputId": "132a0e1b-283b-4feb-eb4f-3c741558852f"
      },
      "source": [
        "import tensorflow as tf\n",
        "gpu_available = tf.test.is_gpu_available()\n",
        "print(gpu_available)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8629qO6i6PhV"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=8192,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tCXTGlM60mr",
        "outputId": "9bb17c87-6248-4e78-a820-0db740420283"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "print('Num parameters: ',model.num_parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num parameters:  49816064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXjtLKSs7QpW"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "# Create the tokenizer from a trained one\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sSOed9JEMms",
        "outputId": "1b298c43-5136-435d-f1fa-cbc1cc34ef19"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='/content/drive/My Drive/Empathy Detection Project/tokenizer', vocab_size=8192, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS8Vgql7Eh48"
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "numeric_feature_names = ['seeker_post', 'response_post']\n",
        "numeric_features = train_df[numeric_feature_names]\n",
        "dataset = tf.data.experimental.make_csv_dataset(\n",
        "  datafile_path,\n",
        "  batch_size=8,\n",
        "  select_columns = [\"seeker_post\",\"response_post\"])\n",
        "#Dataset = tf.data.experimental.make_csv_dataset(datafile_path, batch_size=8)\n",
        "\n",
        "\n",
        "#Dataset = tf.data.Dataset.from_tensor_slices(train_df[\"seeker_post\"],train_df[\"response_post\"])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        # or use the RobertaTokenizer from `transformers` directly.\n",
        "\n",
        "        self.examples = []\n",
        "        \n",
        "        for example in df.values:\n",
        "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
        "            self.examples += [x.input_ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Weâ€™ll pad at the batch level.\n",
        "        return torch.tensor(self.examples[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cX22wzk1vjC"
      },
      "source": [
        "# Create the train and evaluation dataset\n",
        "train_dataset = CustomDataset(train_df['response_post'], tokenizer)\n",
        "eval_dataset = CustomDataset(test_df['response_post'], tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohYZvaxl2TQn"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define the Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5A4CM-Y2hkI",
        "outputId": "b918040f-93c7-43c4-a6ed-c1264c960077"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "print(model_folder)\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_folder,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
        "    save_steps=8192,\n",
        "    #eval_steps=4096,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "# Create the trainer for our model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    #prediction_loss_only=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Empathy Detection Project/pre-training model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lC2Ik_112556",
        "outputId": "ffb49bcd-86b8-456b-8ccd-4af2adc12b91"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 3084\n",
            "  Num Epochs = 13\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5018\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5018' max='5018' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5018/5018 20:36, Epoch 13/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.248062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>6.430000</td>\n",
              "      <td>5.998797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>6.094200</td>\n",
              "      <td>5.811765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.814100</td>\n",
              "      <td>5.747410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.814100</td>\n",
              "      <td>5.656811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.720300</td>\n",
              "      <td>5.599064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.658800</td>\n",
              "      <td>5.623189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.605300</td>\n",
              "      <td>5.599012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.605300</td>\n",
              "      <td>5.416981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.537500</td>\n",
              "      <td>5.419897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.459600</td>\n",
              "      <td>5.343890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.374600</td>\n",
              "      <td>5.264787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>5.343000</td>\n",
              "      <td>5.248071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5018, training_loss=5.703292490904813, metrics={'train_runtime': 1237.2071, 'train_samples_per_second': 32.405, 'train_steps_per_second': 4.056, 'total_flos': 1195687373488128.0, 'train_loss': 5.703292490904813, 'epoch': 13.0})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6e9YxLg080P5",
        "outputId": "f9807625-7dbc-4a1e-d668-83d2552e549c"
      },
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1001\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 198.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtrxHqrJ858O",
        "outputId": "bf1f5e02-baca-484f-9906-e56286fcbb22"
      },
      "source": [
        "trainer.save_model(model_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/My Drive/Empathy Detection Project/pre-training model\n",
            "Configuration saved in /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model weights saved in /content/drive/My Drive/Empathy Detection Project/pre-training model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzR2KyPP9aiN",
        "outputId": "d3418a04-783f-4ac0-f155-787215a99c95"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model_folder,\n",
        "    tokenizer=tokenizer_folder\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/drive/My Drive/Empathy Detection Project/pre-training model\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8192\n",
            "}\n",
            "\n",
            "loading configuration file /content/drive/My Drive/Empathy Detection Project/pre-training model/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/drive/My Drive/Empathy Detection Project/pre-training model\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8192\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/My Drive/Empathy Detection Project/pre-training model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/drive/My Drive/Empathy Detection Project/pre-training model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "Didn't find file /content/drive/My Drive/Empathy Detection Project/tokenizer/added_tokens.json. We won't load it.\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/vocab.json\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/merges.txt\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/special_tokens_map.json\n",
            "loading file /content/drive/My Drive/Empathy Detection Project/tokenizer/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Nn3mHr9rOn",
        "outputId": "7b8ecece-857f-4d86-beb0-89b37a570f45"
      },
      "source": [
        "fill_mask(\"Yeah, this <mask> is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing ... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.08077195286750793,\n",
              "  'sequence': \"Yeah, this, is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 16,\n",
              "  'token_str': ','},\n",
              " {'score': 0.04038098827004433,\n",
              "  'sequence': \"Yeah, this you is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 284,\n",
              "  'token_str': ' you'},\n",
              " {'score': 0.03544344753026962,\n",
              "  'sequence': \"Yeah, this. is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 18,\n",
              "  'token_str': '.'},\n",
              " {'score': 0.029014909639954567,\n",
              "  'sequence': \"Yeah, this to is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 278,\n",
              "  'token_str': ' to'},\n",
              " {'score': 0.027191845700144768,\n",
              "  'sequence': \"Yeah, this a is a strong one. Felt it for a lot of my childhood and early adulthood. The truth is that I never dealt with it constructively, or at least in a way that could help me get to the root of that feeling. Part of the reason I think you and I are on this in the first place is because we were never taught how to deal with that feeling constructively. In my case, I used constant denial and self-numbing... At some point, I just assumed the pit was a part of me I could choose to ignore. You seem to be actively trying to experience it. You are engaging your sadness, which is objectively good for working through depression. It may not be what you might call 'conventional' but everyone does their thing a little differently :) If what you are doing no longer works for you though, it might be best to start looking for other (healthy) ways of working through that discomfort. Personally, when I feel that pit, I know what thoughts tend to follow it. Fortunately, my therapists (and my stay at hospital) helped me come up with ways to attack that pit when it hit me hard. In order to redirect those thoughts that create it, I try to reach out to anyone in my support net. It's hard because I feel like they won't give a shit, but most of the time, they do (but I often choose to ignore that reality). Or I will simply write some positive words here, on this. Reaching out to others here in positive ways is not just for others, but for my own sake as well. As you put so well, giving that pit a 'purpose'. But don't cover that feeling up. Continue to engage with it, as best as you can. It's there for a reason. Hopefully, the more you work with it, the easier it is to process and let go.\",\n",
              "  'token': 263,\n",
              "  'token_str': ' a'}]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}